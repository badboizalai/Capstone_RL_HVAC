{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6344b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SETUP PATH & IMPORTS ====================\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Add project root to path (notebook is in notebooks/)\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"‚úì Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"‚úì Random seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== IMPORT PROJECT MODULES ====================\n",
    "from src.agents import DDPGAgent\n",
    "from src.environments import HVACEnvironment\n",
    "from src.visualization import plot_training_progress\n",
    "from configs.config_manager import get_train_config\n",
    "\n",
    "print(\"‚úì All project modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d4ddd7",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Ch·ªçn ch·∫ø ƒë·ªô training v√† c√°c hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f0b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "# Ch·ªçn mode: True = v·ªõi forecast (state_dim=15), False = kh√¥ng forecast (state_dim=14)\n",
    "USE_FORECAST = False\n",
    "\n",
    "# Get config t·ª´ config_manager\n",
    "config = get_train_config(use_forecast=USE_FORECAST)\n",
    "\n",
    "# Override settings n·∫øu c·∫ßn\n",
    "config.NUM_EPISODES = 50\n",
    "config.SAVE_FREQ = 2\n",
    "config.BATCH_SIZE = 512\n",
    "\n",
    "# Override paths n·∫øu c·∫ßn\n",
    "config.FMU_PATH = str(PROJECT_ROOT / \"HVAC.fmu\")\n",
    "config.WEATHER_CSV = str(PROJECT_ROOT / \"data\" / \"weather_data.csv\")  # Thay ƒë·ªïi path\n",
    "\n",
    "# Create directories\n",
    "config.create_directories()\n",
    "\n",
    "# Display config\n",
    "config.display()\n",
    "print(f\"\\nüìÅ Paths:\")\n",
    "print(f\"   FMU:         {config.FMU_PATH}\")\n",
    "print(f\"   Weather:     {config.WEATHER_CSV}\")\n",
    "print(f\"   Checkpoints: {config.CHECKPOINT_PATH}\")\n",
    "print(f\"   Results:     {config.RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ff6b1",
   "metadata": {},
   "source": [
    "## 2. Initialize Environment & Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== INITIALIZE ====================\n",
    "print(\"=\"*70)\n",
    "print(\"üîß INITIALIZING ENVIRONMENT & AGENT\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Initialize Environment t·ª´ src.environments\n",
    "env = HVACEnvironment(config, verbose=False)\n",
    "\n",
    "# Initialize DDPG Agent t·ª´ src.agents\n",
    "agent = DDPGAgent(config)\n",
    "\n",
    "print(f\"\\n‚úÖ Environment: {type(env).__name__}\")\n",
    "print(f\"‚úÖ Agent: {type(agent).__name__}\")\n",
    "print(f\"   State dim:  {config.STATE_DIM}\")\n",
    "print(f\"   Action dim: {config.ACTION_DIM}\")\n",
    "print(f\"   Device:     {config.DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913828ea",
   "metadata": {},
   "source": [
    "## 3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90137168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== TRAINING LOOP ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ STARTING DDPG TRAINING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Tracking\n",
    "episode_rewards = []\n",
    "episode_stats = []\n",
    "best_reward = -np.inf\n",
    "\n",
    "VERBOSE_STEP = False  # Set True ƒë·ªÉ xem chi ti·∫øt t·ª´ng step\n",
    "\n",
    "for episode in range(1, config.NUM_EPISODES + 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä Episode {episode}/{config.NUM_EPISODES}\")\n",
    "    \n",
    "    # Show learning rates\n",
    "    lrs = agent.get_current_lr()\n",
    "    print(f\"   LR: Actor={lrs['actor_lr']:.2e}  Critic={lrs['critic_lr']:.2e}\")\n",
    "    print(f\"   Exploration: Œµ={agent.epsilon:.3f}  œÉ={agent.noise.get_sigma():.3f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Reset environment\n",
    "    state = env.reset(episode=episode)\n",
    "    agent.noise.reset()\n",
    "    \n",
    "    episode_reward = 0.0\n",
    "    step_count = 0\n",
    "    force_explore = episode <= 3  # Force exploration early\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Select action\n",
    "        action = agent.select_action(state, add_noise=True, force_explore=force_explore)\n",
    "        \n",
    "        # Execute step\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Store transition\n",
    "        agent.store_transition(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Train agent\n",
    "        actor_loss, critic_loss = agent.train()\n",
    "        \n",
    "        episode_reward += reward\n",
    "        step_count += 1\n",
    "        state = next_state\n",
    "        \n",
    "        # Verbose output\n",
    "        if VERBOSE_STEP and step_count % 100 == 0 and info:\n",
    "            print(f\"  Step {step_count:04d} | T={info['T_zone']:.2f}¬∞C | \"\n",
    "                  f\"RH={info['RH_zone']:.3f} | P={info['P_total']/1000:.2f}kW | r={reward:.3f}\")\n",
    "    \n",
    "    # Update learning rate\n",
    "    agent.update_learning_rate(episode_reward)\n",
    "    \n",
    "    # Get episode statistics\n",
    "    stats = env.get_episode_stats()\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_stats.append(stats)\n",
    "    \n",
    "    # Print episode summary\n",
    "    print(f\"\\nüìà Episode {episode} Summary:\")\n",
    "    print(f\"   Total Reward:     {episode_reward:.2f}\")\n",
    "    print(f\"   Steps:            {step_count}\")\n",
    "    print(f\"   Avg Temperature:  {stats['avg_T']:.2f}¬∞C ¬± {stats['std_T']:.2f}¬∞C\")\n",
    "    print(f\"   Avg Humidity:     {stats['avg_RH']:.3f} ¬± {stats['std_RH']:.3f}\")\n",
    "    print(f\"   T Comfort:        {stats['T_comfort_ratio']*100:.1f}%\")\n",
    "    print(f\"   RH Comfort:       {stats['RH_comfort_ratio']*100:.1f}%\")\n",
    "    print(f\"   Action Diversity: {stats['action_diversity']:.4f}\")\n",
    "    print(f\"   Buffer Size:      {agent.replay_buffer.size()}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if episode_reward > best_reward:\n",
    "        best_reward = episode_reward\n",
    "        best_path = config.CHECKPOINT_PATH / \"best_model.pth\"\n",
    "        agent.save(best_path)\n",
    "        print(f\"   ‚≠ê NEW BEST MODEL! Saved to {best_path}\")\n",
    "    \n",
    "    # Periodic save\n",
    "    if episode % config.SAVE_FREQ == 0:\n",
    "        checkpoint_path = config.CHECKPOINT_PATH / f\"model_ep{episode}.pth\"\n",
    "        agent.save(checkpoint_path)\n",
    "        print(f\"   üíæ Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training loop completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac86b00",
   "metadata": {},
   "source": [
    "## 4. Save Final Model & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ceb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SAVE FINAL MODEL ====================\n",
    "# Save final model\n",
    "final_path = config.CHECKPOINT_PATH / \"final_model.pth\"\n",
    "agent.save(final_path)\n",
    "print(f\"‚úì Final model saved: {final_path}\")\n",
    "\n",
    "# Save training history\n",
    "history = {\n",
    "    'episode_rewards': [float(r) for r in episode_rewards],\n",
    "    'episode_stats': episode_stats,\n",
    "    'config': {\n",
    "        'use_forecast': config.USE_FORECAST,\n",
    "        'state_dim': config.STATE_DIM,\n",
    "        'action_dim': config.ACTION_DIM,\n",
    "        'num_episodes': config.NUM_EPISODES,\n",
    "        'lr_actor': config.LR_ACTOR,\n",
    "        'lr_critic': config.LR_CRITIC,\n",
    "        'gamma': config.GAMMA,\n",
    "        'tau': config.TAU\n",
    "    }\n",
    "}\n",
    "\n",
    "history_path = config.RESULTS_PATH / \"training_history.json\"\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(f\"‚úì Training history saved: {history_path}\")\n",
    "\n",
    "# Save stats to CSV\n",
    "stats_df = pd.DataFrame(episode_stats)\n",
    "stats_df['episode'] = range(1, len(episode_stats) + 1)\n",
    "stats_df['total_reward'] = episode_rewards\n",
    "stats_csv = config.RESULTS_PATH / \"training_stats.csv\"\n",
    "stats_df.to_csv(stats_csv, index=False)\n",
    "print(f\"‚úì Training stats saved: {stats_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e52fc96",
   "metadata": {},
   "source": [
    "## 5. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce538dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== TRAINING SUMMARY ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Mode:              {'WITH Forecast' if config.USE_FORECAST else 'NO Forecast'}\")\n",
    "print(f\"  Total Episodes:    {len(episode_rewards)}\")\n",
    "print(f\"  Best Reward:       {best_reward:.2f}\")\n",
    "print(f\"  Final Reward:      {episode_rewards[-1]:.2f}\")\n",
    "print(f\"  Avg Last 5 Ep:     {np.mean(episode_rewards[-5:]):.2f}\")\n",
    "print(f\"\\n  Final Stats:\")\n",
    "print(f\"    Avg Temperature: {episode_stats[-1]['avg_T']:.2f}¬∞C\")\n",
    "print(f\"    T Comfort:       {episode_stats[-1]['T_comfort_ratio']*100:.1f}%\")\n",
    "print(f\"    Avg Humidity:    {episode_stats[-1]['avg_RH']:.3f}\")\n",
    "print(f\"    RH Comfort:      {episode_stats[-1]['RH_comfort_ratio']*100:.1f}%\")\n",
    "print(f\"\\nüìÅ Checkpoints: {config.CHECKPOINT_PATH}\")\n",
    "print(f\"üìÅ Results:     {config.RESULTS_PATH}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73ebcef",
   "metadata": {},
   "source": [
    "## 6. Plot Training Progress\n",
    "\n",
    "S·ª≠ d·ª•ng `plot_training_progress` t·ª´ `src.visualization`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== PLOT TRAINING PROGRESS ====================\n",
    "# S·ª≠ d·ª•ng function t·ª´ src.visualization\n",
    "plot_training_progress(episode_rewards, episode_stats, config.RESULTS_PATH)\n",
    "\n",
    "print(f\"\\n‚úÖ Training curves saved to: {config.RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== DISPLAY TRAINING PLOT ====================\n",
    "from IPython.display import Image, display\n",
    "\n",
    "plot_path = config.RESULTS_PATH / \"training_progress.png\"\n",
    "if plot_path.exists():\n",
    "    print(\"üìä Training Progress:\")\n",
    "    display(Image(filename=str(plot_path), width=900))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c66bdc6",
   "metadata": {},
   "source": [
    "## 7. Quick Test Best Model (Optional)\n",
    "\n",
    "Test nhanh best model ƒë·ªÉ ki·ªÉm tra action diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeac14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== QUICK TEST ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß™ QUICK TEST - Best Model (100 steps)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Load best model\n",
    "test_agent = DDPGAgent(config)\n",
    "test_agent.load(config.CHECKPOINT_PATH / \"best_model.pth\")\n",
    "print(f\"‚úì Loaded best model\")\n",
    "\n",
    "# Quick test\n",
    "test_state = env.reset()\n",
    "test_actions = []\n",
    "\n",
    "for i in range(100):\n",
    "    action = test_agent.select_action(test_state, add_noise=False)  # No noise\n",
    "    test_actions.append(action.copy())\n",
    "    next_state, _, done, _ = env.step(action)\n",
    "    test_state = next_state\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# Check action diversity\n",
    "test_actions = np.array(test_actions)\n",
    "action_diversity = np.mean(np.std(test_actions, axis=0))\n",
    "\n",
    "print(f\"\\nüîç Action Analysis (100 steps, no noise):\")\n",
    "print(f\"   Diversity (std):  {action_diversity:.4f}\")\n",
    "print(f\"   uFan:    mean={np.mean(test_actions[:, 0]):.3f}, std={np.std(test_actions[:, 0]):.3f}\")\n",
    "print(f\"   uOA:     mean={np.mean(test_actions[:, 1]):.3f}, std={np.std(test_actions[:, 1]):.3f}\")\n",
    "print(f\"   uChiller: mean={np.mean(test_actions[:, 2]):.3f}, std={np.std(test_actions[:, 2]):.3f}\")\n",
    "print(f\"   uHeater: mean={np.mean(test_actions[:, 3]):.3f}, std={np.std(test_actions[:, 3]):.3f}\")\n",
    "print(f\"   uFanEA:  mean={np.mean(test_actions[:, 4]):.3f}, std={np.std(test_actions[:, 4]):.3f}\")\n",
    "\n",
    "if action_diversity < 0.01:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Very low diversity - model might be STUCK!\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Action diversity is healthy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33747a29",
   "metadata": {},
   "source": [
    "## 8. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c71c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== FINAL SUMMARY ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìÅ Output Files:\")\n",
    "print(f\"   Best Model:    {config.CHECKPOINT_PATH / 'best_model.pth'}\")\n",
    "print(f\"   Final Model:   {config.CHECKPOINT_PATH / 'final_model.pth'}\")\n",
    "print(f\"   History JSON:  {config.RESULTS_PATH / 'training_history.json'}\")\n",
    "print(f\"   Stats CSV:     {config.RESULTS_PATH / 'training_stats.csv'}\")\n",
    "print(f\"   Plot:          {config.RESULTS_PATH / 'training_progress.png'}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
